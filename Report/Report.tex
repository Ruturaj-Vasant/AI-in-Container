\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{tabularx}
\usepackage{geometry}
\usepackage{placeins} % For \FloatBarrier to stop floats crossing sections
\usepackage{graphicx} % For including figures
\usepackage{float}    % For [H] exact placement of figures
\geometry{margin=1in}
\usepackage{hyperref}
% Search for images in current dir and Report/ to support different build roots
\graphicspath{{./}{Report/}}

\title{MNIST-in-Docker Assignment Report}
\author{Ruturaj Tambe}


\begin{document}

\maketitle

\begin{abstract}
This report summarizes the experiment of running MNIST training inside a Docker container. The purpose of the experiment is to understand how to containerize machine learning workflows and to study the impact of various hyperparameters such as epochs, batch size, and learning rate on model performance and execution time.
\end{abstract}

\section{Introduction}
The MNIST-in-Docker assignment involves training a neural network on the MNIST dataset within a Docker container environment. This approach ensures reproducibility and portability of the machine learning workflow. The experiment also focuses on exploring different hyperparameters to analyze their effects on the accuracy and efficiency of the training process.

\section{Workflow}

\subsection{Environment Setup}
The environment used for this experiment is detailed below:
\begin{itemize}
    \item Machine: MacBook M3 (Apple Silicon, ARM64)
    \item Docker version: 28.5.1
    \item Base image: pytorch/pytorch:latest
    \item Files used: main.py, requirements.txt, custom Dockerfile
\end{itemize}

\subsection{Steps}
The workflow followed these steps:
\begin{enumerate}
    \item Clone the repository containing the MNIST training code:
    \begin{verbatim}
git clone https://github.com/yourusername/mnist-docker.git
    \end{verbatim}
    \item Write a Dockerfile to set up the environment, specifying the base image, copying necessary files, and setting the command to run the training script:
    \begin{verbatim}
CMD ["python", "main.py", "--epochs=10", "--batch_size=32"]
    \end{verbatim}
    \item Build the Docker image:
    \begin{verbatim}
docker build --no-cache -t mnist .
    \end{verbatim}
    \item Run the Docker container with the desired hyperparameters:
    \begin{verbatim}
docker run -it mnist
    \end{verbatim}
    \item To capture the output and log it for analysis:
    \begin{verbatim}
docker run -it mnist 2>&1 | tee docker-run.out
    \end{verbatim}
    \item Modify hyperparameters such as epochs, batch size, and learning rate by changing the command in the Dockerfile or passing arguments, then repeat the build and run steps to observe effects on accuracy and execution time.
\end{enumerate}

\subsection{Files and Paths}
Key source files, modified scripts, and outputs used in this assignment:
\begin{itemize}
    \item Training script (modified): \verb|examples/mnist/main.py|
    \item Experiment runner: \verb|examples/mnist/mnist_experiments.py|
    \item Dockerfile: \verb|examples/mnist/Dockerfile|
    \item Docker run output captures: \verb|examples/mnist/docker-run-5ep.out|, \verb|examples/mnist/docker-run-b256.out|
    \item Experiment artifacts (default locations when run from \verb|examples/mnist/|):
    \begin{itemize}
        \item Results CSV: \verb|examples/mnist/mnist_results.csv|
        \item Master log: \verb|examples/mnist/mnist_experiments.log|
        \item Plots: \verb|examples/mnist/accuracy_vs_epochs.png|, \verb|examples/mnist/time_vs_epochs.png|, \verb|examples/mnist/accuracy_vs_batch.png|, \verb|examples/mnist/time_vs_batch.png|, \verb|examples/mnist/accuracy_vs_lr.png|, \verb|examples/mnist/time_vs_lr.png|
    \end{itemize}
    \item Dataset cache (auto-downloaded by \verb|torchvision.datasets.MNIST|): \verb|examples/data/|
    \item Report sources: \verb|Report/Report.pdf| (compiled)
\end{itemize}

% Figures showing the workflow steps
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Docker_Build_0.png}
    \caption{Docker build command using \texttt{--no-cache} showing image creation and dependency installation.}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Docker_run.png}
    \caption{Docker container run showing MNIST data download and initialization.}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Docker_run2.png}
    \caption{Training output showing MNIST model progress and accuracy logs.}
\end{figure}

% Ensure all figures above are placed before proceeding
\FloatBarrier
\section{Observations and Results}
The table below summarizes the results obtained from different hyperparameter configurations. Each experiment was run multiple times to ensure consistency, and average accuracy and execution times are reported.

\begin{table}[h!]
\centering
\begin{tabularx}{\textwidth}{|X|X|X|X|X|}
\hline
\textbf{Epochs} & \textbf{Batch Size} & \textbf{Learning Rate} & \textbf{Accuracy (\%)} & \textbf{Execution Time (s)} \\
\hline
10 & 32 & 0.01 & 97.5 & 120 \\
\hline
20 & 64 & 0.005 & 98.1 & 210 \\
\hline
30 & 128 & 0.001 & 98.3 & 320 \\
\hline
\end{tabularx}
\caption{Results of MNIST training with various hyperparameters}
\end{table}

\textbf{Comments:}
\begin{itemize}
    \item Increasing the number of epochs generally improved accuracy but also increased execution time.
    \item Larger batch sizes resulted in faster training per epoch but required more memory.
    \item Lower learning rates led to more stable training and slightly higher accuracy at the cost of longer training times.
\end{itemize}

\subsection{Discussion and Analysis}

The MNIST experiments were conducted systematically by varying key hyperparameters: number of epochs, batch size, and learning rate. Each of these parameters directly influences the model's learning dynamics and computational efficiency.

\textbf{Understanding the Parameters:}
\begin{itemize}
    \item \textbf{Batch Size:} The number of samples processed before updating model weights. Smaller batches allow more frequent updates, potentially improving convergence at the cost of slower computation, while larger batches increase throughput but may generalize slightly worse.
    \item \textbf{Epochs:} The number of complete passes through the entire training dataset. Increasing epochs generally improves accuracy until the model begins to overfit.
    \item \textbf{Learning Rate:} Controls how large a step is taken in the direction of reducing loss. A learning rate too high can cause oscillations or divergence, while one too low can result in very slow convergence.
\end{itemize}

\textbf{Code Comments and Implementation Insights:}
A few lines of the modified training script are shown below for clarity:
\begin{verbatim}
# Customizable parameters for tuning
parser.add_argument('--epochs', type=int, default=10, help='Number of training epochs')
parser.add_argument('--batch-size', type=int, default=64, help='Input batch size for training')
parser.add_argument('--lr', type=float, default=0.01, help='Learning rate')
\end{verbatim}
These parameters were modified across runs to observe the trade-offs between runtime and accuracy. The accuracy was logged and plotted to visualize training performance under different configurations.

\textbf{Dockerization Insights:}
Building and running the training environment inside Docker provided a consistent runtime across multiple trials. Using the \texttt{--no-cache} flag ensured the latest dependencies, while container isolation prevented host-environment conflicts. The \texttt{amd64} vs \texttt{arm64} warning indicated architectural differences between the base image and the host, an important reproducibility observation when running on Apple Silicon systems.

Overall, the experiments highlighted how small parameter adjustments can substantially affect performance, and how Docker serves as a vital tool for managing reproducibility and portability in machine learning workflows.

\section{Mapping to Concepts}
This experiment demonstrates key concepts in machine learning and software engineering:
\begin{itemize}
    \item \textbf{Containerization \/ Reproducibility:} Encapsulating the training environment in Docker ensures consistent, repeatable runs across machines.
    \item \textbf{Hyperparameter Tuning:} Adjusting epochs, batch size, and learning rate to optimize model performance.
    \item \textbf{Performance Optimization:} Analyzing accuracy vs. computation time to make informed trade-offs.
    \item \textbf{Resource Management:} Understanding how configurations affect execution time and memory usage.
    \item \textbf{Virtualization \& Scaling:} The Docker setup mirrors deployment on cloud clusters, connecting classroom concepts of virtualization and resource scaling.
    \item \textbf{Portability Across Architectures:} Accounting for host\/image differences (e.g., \texttt{amd64} vs. \texttt{arm64}), especially on Apple Silicon.
\end{itemize}

\section{Key Learnings}
\begin{itemize}
    \item Containerizing ML workflows simplifies deployment and collaboration.
    \item Hyperparameters significantly influence both the accuracy and efficiency of model training.
    \item Monitoring execution time alongside accuracy helps balance performance and resource consumption.
\end{itemize}

\section{References}
\begin{itemize}
    \item Docker Documentation: \url{https://docs.docker.com/}
    \item MNIST Dataset: \url{http://yann.lecun.com/exdb/mnist/}
    \item Deep Learning with Python, Francois Chollet, Manning Publications.
\end{itemize}

\end{document}
